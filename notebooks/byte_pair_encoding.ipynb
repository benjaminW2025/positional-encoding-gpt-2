{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a64cd69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../files/my_object.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# create tokenizer and train\u001b[39;00m\n\u001b[32m     32\u001b[39m tokenizer = BytePairTokenizer(full_training_corpus)\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m4744\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# save the trained tokenizer\u001b[39;00m\n\u001b[32m     36\u001b[39m tokenizer_state = {\n\u001b[32m     37\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmerges\u001b[39m\u001b[33m'\u001b[39m: tokenizer.merges,\n\u001b[32m     38\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtoken_to_id\u001b[39m\u001b[33m'\u001b[39m: tokenizer.token_ids,\n\u001b[32m     39\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mbyte_to_char\u001b[39m\u001b[33m'\u001b[39m: tokenizer.byte_to_char\n\u001b[32m     40\u001b[39m }   \n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/coding/positional-gpt-2/data/byte_pair_encoding.py:33\u001b[39m, in \u001b[36mBytePairTokenizer.train\u001b[39m\u001b[34m(self, num_merges)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# set up training loop\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m iw \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_merges):\n\u001b[32m     32\u001b[39m     \u001b[38;5;66;03m# get the pairwise stats\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m     stats = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_stats\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcurrent_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m     \u001b[38;5;66;03m# if no more pairs to merge\u001b[39;00m\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stats:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/coding/positional-gpt-2/data/byte_pair_encoding.py:120\u001b[39m, in \u001b[36mBytePairTokenizer.get_stats\u001b[39m\u001b[34m(self, tokens)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;66;03m# initiate the loop\u001b[39;00m\n\u001b[32m    119\u001b[39m i = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m (i < length - \u001b[32m1\u001b[39m):\n\u001b[32m    121\u001b[39m     curr = (tokens[i], tokens[i+\u001b[32m1\u001b[39m])\n\u001b[32m    122\u001b[39m     pairs_dict[curr] = pairs_dict.get(curr, \u001b[32m0\u001b[39m) + \u001b[32m1\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Train and save the tokenizer\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(\"/Users/benjawesome/coding/positional-gpt-2\")\n",
    "\n",
    "save_dir = os.path.join(\"..\", \"files\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "save_path = os.path.join(save_dir, \"my_object.pkl\")\n",
    "\n",
    "print(save_path)\n",
    "\n",
    "from data.byte_pair_encoding import BytePairTokenizer\n",
    "from datasets import load_dataset\n",
    "import pickle\n",
    "\n",
    "raw_datasets = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "\n",
    "# get our data\n",
    "train_text_list = raw_datasets[\"train\"][\"text\"]\n",
    "val_text_list = raw_datasets[\"validation\"][\"text\"]\n",
    "\n",
    "# turn the data into strings for input\n",
    "full_training_corpus = \"\\n\".join(train_text_list)\n",
    "val_training_corpus = \"\\n\".join(val_text_list)\n",
    "\n",
    "# create tokenizer and train\n",
    "tokenizer = BytePairTokenizer(full_training_corpus)\n",
    "tokenizer.train(4744)\n",
    "\n",
    "# save the trained tokenizer\n",
    "tokenizer_state = {\n",
    "    'merges': tokenizer.merges,\n",
    "    'token_to_id': tokenizer.token_ids,\n",
    "    'byte_to_char': tokenizer.byte_to_char\n",
    "}   \n",
    "\n",
    "with open(save_path, 'wb') as f:\n",
    "    pickle.dump(tokenizer_state, f)\n",
    "\n",
    "# test encoding on val set\n",
    "encoded = tokenizer.encode(val_training_corpus[:1000])\n",
    "\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d08d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comma id: 11\n",
      "11\n",
      "11\n",
      "[373, 298, 39, 315, 271, 630, 1041, 748, 630, 1649, 39, 315, 271, 630, 1041, 748, 2712, 3198, 2684, 75, 597, 2833, 312, 1998, 75, 597, 290, 588, 919, 1323, 278, 407, 585, 268, 75, 597, 2833, 612, 2552, 2835, 1174, 1709, 266, 44, 3152, 261, 535, 1709, 2081, 1188, 448, 3777, 3226, 2081, 279, 2296, 1744, 584, 4275, 418, 1113, 75, 597, 290, 588, 39, 270, 318, 261, 294, 265, 630, 709, 1110, 70, 2111, 1159, 2813, 3195, 2262, 66, 1861, 2202, 2574, 1455, 3322, 542, 517, 3781, 502, 81, 2921, 362, 1811, 75, 926, 2655, 358, 271, 1702, 359, 420, 294, 84, 618, 4464, 278, 407, 585, 257, 540, 1070, 1200, 75, 597, 290, 390, 455, 505, 84, 370, 736, 2376, 2135, 75, 597, 2833, 1153, 308, 284, 66, 419, 830, 844, 885, 2016, 3349, 4674, 588, 852, 286, 670, 70, 257, 456, 455, 2514, 1049, 2155, 2873, 351, 706, 1159, 1096, 915, 71, 292, 2732, 848, 340, 1814, 3094, 434, 744, 85, 3959, 487, 315, 271, 630, 1041, 748, 630, 919, 4311, 504, 68, 339, 268, 3492, 457, 301, 4441, 66, 4061, 2347, 75, 597, 2833, 489, 2051, 3573, 2983, 1163, 1338, 945, 4086, 316, 640, 79, 324, 643, 39, 315, 271, 630, 1041, 748, 630, 919, 1314, 3364, 290, 313, 1709, 266, 902, 1971, 4965, 2695, 2262, 66, 281, 484, 383, 2341, 2202, 285, 1560, 86, 2237, 286, 2695, 4831, 517, 3781, 502, 81, 2921, 362, 1706, 527, 1811, 75, 926, 735, 1498, 263, 75, 597, 290, 390, 66, 4061, 285, 75, 597, 2833, 489, 421, 455, 3108, 2252, 527, 3272, 66, 1861, 603, 527, 1477, 2574, 1202, 280, 2644, 3418, 371, 577, 31]\n",
      "5001\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load and test the tokenizer object\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "sys.path.append(\"/Users/benjawesome/coding/positional-gpt-2\")\n",
    "\n",
    "save_dir = os.path.join(\"..\", \"files\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "save_path = os.path.join(save_dir, \"my_object.pkl\")\n",
    "\n",
    "from data.byte_pair_encoding import BytePairTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "tokenizer = BytePairTokenizer.load(save_path)\n",
    "\n",
    "print(\"comma id: \" + str(tokenizer.token_ids.get(\",\")))\n",
    "\n",
    "raw_datasets = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "\n",
    "# get our data\n",
    "train_text_list = raw_datasets[\"train\"][\"text\"]\n",
    "val_text_list = raw_datasets[\"validation\"][\"text\"]\n",
    "\n",
    "# turn the data into strings for input\n",
    "full_training_corpus = \"\\n\".join(train_text_list)\n",
    "val_training_corpus = \"\\n\".join(val_text_list)\n",
    "\n",
    "print(token_ids.get(\",\"))\n",
    "\n",
    "# realized that my tokenizer didn't handle punctuation correct\n",
    "\n",
    "# fix the missing puncutations\n",
    "\n",
    "MISSING_TOKENS = [\",\", \".\", \"!\", \"?\", \" \"]\n",
    "\n",
    "next_available_id = 5000\n",
    "\n",
    "for token_string in MISSING_TOKENS:\n",
    "    if token_string not in tokenizer.token_ids:\n",
    "        tokenizer.token_ids[token_string] = next_available_id\n",
    "        next_available_id += 1\n",
    "        print(f\"Assigned ID {tokenizer.token_ids[token_string]} to token: '{token_string}'\")\n",
    "\n",
    "tokenizer_state = {\n",
    "    'merges': tokenizer.merges,\n",
    "    'token_to_id': tokenizer.token_ids,\n",
    "    'byte_to_char': tokenizer.byte_to_char\n",
    "}   \n",
    "\n",
    "with open(save_path, 'wb') as f:\n",
    "    pickle.dump(tokenizer_state, f)\n",
    "\n",
    "print(tokenizer.token_ids.get(\",\"))\n",
    "\n",
    "encoded = tokenizer.encode(val_training_corpus[:1000])\n",
    "\n",
    "print(encoded)\n",
    "\n",
    "print(len(tokenizer.token_ids))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
